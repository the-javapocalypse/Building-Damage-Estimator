{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "source": [
    "# Importing Dependencies\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Utilities.model_visualization import model_to_png\n",
    "\n",
    "print('Imports Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to training and validation data\n",
    "\n",
    "train_data_dir = 'dataset/train'\n",
    "validation_data_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params for CNN\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 5\n",
    "epochs = 200\n",
    "train_samples = 420\n",
    "validation_samples = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading vgg except the final layer\n",
    "\n",
    "vgg_conv = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 16,944,194\n",
      "Trainable params: 9,308,930\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_top = Sequential()\n",
    "# Add the vgg convolutional base model\n",
    "model_top.add(vgg_conv)\n",
    "\n",
    "model_top.add(Flatten())\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.2))\n",
    "model_top.add(Dense(2, activation='sigmoid'))\n",
    "model_top.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_top.summary()\n",
    "\n",
    "model_to_png(model_top, 'Fine_Tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 420 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_batchsize = 210\n",
    "val_batchsize = 40\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical') # class_mode='categorical'\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical', # class_mode='categorical'\n",
    "        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.7475 - acc: 0.5036 - val_loss: 0.6754 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.57500, saving model to model_finetuned.h5\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6841 - acc: 0.5524 - val_loss: 0.6221 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.57500 to 0.70000, saving model to model_finetuned.h5\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5828 - acc: 0.6833 - val_loss: 0.5588 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.70000 to 0.71875, saving model to model_finetuned.h5\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.7632 - acc: 0.6393 - val_loss: 0.6279 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5867 - acc: 0.6833 - val_loss: 0.5722 - val_acc: 0.7437\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.71875 to 0.74375, saving model to model_finetuned.h5\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5134 - acc: 0.7298 - val_loss: 0.4766 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.74375 to 0.78750, saving model to model_finetuned.h5\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.4921 - acc: 0.7571 - val_loss: 0.5712 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6629 - acc: 0.6286 - val_loss: 0.6142 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5284 - acc: 0.7405 - val_loss: 0.4852 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.4281 - acc: 0.7929 - val_loss: 0.4162 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.78750 to 0.79375, saving model to model_finetuned.h5\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.4200 - acc: 0.8036 - val_loss: 0.5998 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5067 - acc: 0.7607 - val_loss: 0.4475 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.79375 to 0.80000, saving model to model_finetuned.h5\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3883 - acc: 0.8381 - val_loss: 0.3802 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.80000 to 0.84375, saving model to model_finetuned.h5\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3826 - acc: 0.8393 - val_loss: 0.4907 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5732 - acc: 0.7119 - val_loss: 0.4903 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.4022 - acc: 0.8548 - val_loss: 0.3751 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84375 to 0.87500, saving model to model_finetuned.h5\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3440 - acc: 0.8619 - val_loss: 0.3529 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.87500 to 0.88125, saving model to model_finetuned.h5\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3443 - acc: 0.8583 - val_loss: 0.4730 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5103 - acc: 0.7536 - val_loss: 0.4413 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3364 - acc: 0.8690 - val_loss: 0.3322 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.2903 - acc: 0.8821 - val_loss: 0.3642 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3274 - acc: 0.8571 - val_loss: 0.4382 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3494 - acc: 0.8512 - val_loss: 0.4030 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3193 - acc: 0.8524 - val_loss: 0.3873 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.3103 - acc: 0.8762 - val_loss: 0.3668 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Compile the model\n",
    "model_top.compile(loss='binary_crossentropy', #categorical_crossentropy\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"model_finetuned.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=8, verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "#Train the model\n",
    "history = model_top.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=60,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1,\n",
    "      callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_top.save_weights('fine-tined-8812.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "prediction_data_dir = 'dataset/predict'\n",
    "# Create a generator for prediction\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        prediction_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/0 [====================================================================================================] - 1s 1s/step\n",
      "No of errors = 0/12\n"
     ]
    }
   ],
   "source": [
    "# Get the filenames from the generator\n",
    "fnames = validation_generator.filenames\n",
    " \n",
    "# Get the ground truth from generator\n",
    "ground_truth = validation_generator.classes\n",
    " \n",
    "# Get the label to class mapping from the generator\n",
    "label2index = validation_generator.class_indices\n",
    " \n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    " \n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model_top.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
    "predicted_classes = np.argmax(predictions,axis=1)\n",
    " \n",
    "errors = np.where(predicted_classes != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the errors\n",
    "for i in range(len(errors)):\n",
    "    pred_class = np.argmax(predictions[errors[i]])\n",
    "    pred_label = idx2label[pred_class]\n",
    "     \n",
    "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "        fnames[errors[i]].split('/')[0],\n",
    "        pred_label,\n",
    "        predictions[errors[i]][pred_class])\n",
    "     \n",
    "    original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(original)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
