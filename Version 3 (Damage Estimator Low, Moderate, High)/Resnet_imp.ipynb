{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javapocalypse\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math, json, os, sys\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Utilities.model_visualization import model_to_png\n",
    "\n",
    "\n",
    "print('Imports Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to training and validation data\n",
    "\n",
    "train_data_dir = 'dataset/train'\n",
    "validation_data_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params for CNN\n",
    "\n",
    "# img_width, img_height = 150, 150\n",
    "# batch_size = 5\n",
    "# epochs = 200\n",
    "# train_samples = 420\n",
    "# validation_samples = 80\n",
    "# img_channels = 3\n",
    "# cardinality = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'dataset_new'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'test')\n",
    "SIZE = (224, 224)\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])\n",
    "num_valid_samples = sum([len(files) for r, d, files in os.walk(VALID_DIR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_steps = math.floor(num_train_samples/BATCH_SIZE)\n",
    "num_valid_steps = math.floor(num_valid_samples/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = keras.preprocessing.image.ImageDataGenerator()\n",
    "val_gen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9013 images belonging to 3 classes.\n",
      "Found 2324 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = gen.flow_from_directory(\n",
    "    TRAIN_DIR, \n",
    "    target_size=SIZE, \n",
    "    class_mode='categorical', \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "val_batches = val_gen.flow_from_directory(\n",
    "    VALID_DIR, \n",
    "    target_size=SIZE, \n",
    "    class_mode='categorical', \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.applications.resnet50.ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = list(iter(batches.class_indices))\n",
    "\n",
    "model.layers.pop()\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "last = model.layers[-1].output\n",
    "\n",
    "x = Dense(len(classes), activation=\"softmax\")(last)\n",
    "\n",
    "finetuned_model = Model(model.input, x)\n",
    "\n",
    "finetuned_model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for c in batches.class_indices:\n",
    "    classes[batches.class_indices[c]] = c\n",
    "    \n",
    "finetuned_model.classes = classes\n",
    "\n",
    "model_to_png(finetuned_model, 'Fine_Tuned_Resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "checkpointer = ModelCheckpoint('resnet50_best.h5', verbose=1, save_best_only=True, monitor='val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "180/180 [==============================] - 286s 2s/step - loss: 0.8071 - acc: 0.6575 - val_loss: 1.3187 - val_acc: 0.4722\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.47217, saving model to resnet50_best.h5\n",
      "Epoch 2/100\n",
      "180/180 [==============================] - 136s 756ms/step - loss: 0.6191 - acc: 0.7576 - val_loss: 1.3467 - val_acc: 0.4757\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.47217 to 0.47565, saving model to resnet50_best.h5\n",
      "Epoch 3/100\n",
      "180/180 [==============================] - 136s 756ms/step - loss: 0.5716 - acc: 0.7773 - val_loss: 1.3062 - val_acc: 0.4865\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.47565 to 0.48652, saving model to resnet50_best.h5\n",
      "Epoch 4/100\n",
      "180/180 [==============================] - 135s 749ms/step - loss: 0.5402 - acc: 0.7902 - val_loss: 1.3711 - val_acc: 0.4822\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/100\n",
      "180/180 [==============================] - 135s 750ms/step - loss: 0.5248 - acc: 0.7969 - val_loss: 1.2434 - val_acc: 0.5113\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.48652 to 0.51130, saving model to resnet50_best.h5\n",
      "Epoch 6/100\n",
      "180/180 [==============================] - 140s 775ms/step - loss: 0.5052 - acc: 0.8032 - val_loss: 1.2778 - val_acc: 0.5087\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/100\n",
      "180/180 [==============================] - 139s 770ms/step - loss: 0.4962 - acc: 0.8092 - val_loss: 1.3021 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/100\n",
      "180/180 [==============================] - 137s 760ms/step - loss: 0.4889 - acc: 0.8105 - val_loss: 1.2372 - val_acc: 0.5161\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.51130 to 0.51609, saving model to resnet50_best.h5\n",
      "Epoch 9/100\n",
      "180/180 [==============================] - 150s 831ms/step - loss: 0.4793 - acc: 0.8152 - val_loss: 1.2176 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.51609 to 0.52478, saving model to resnet50_best.h5\n",
      "Epoch 10/100\n",
      "180/180 [==============================] - 134s 743ms/step - loss: 0.4698 - acc: 0.8182 - val_loss: 1.2319 - val_acc: 0.5387\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.52478 to 0.53870, saving model to resnet50_best.h5\n",
      "Epoch 11/100\n",
      "180/180 [==============================] - 134s 742ms/step - loss: 0.4570 - acc: 0.8218 - val_loss: 1.2579 - val_acc: 0.5235\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/100\n",
      "180/180 [==============================] - 133s 738ms/step - loss: 0.4584 - acc: 0.8204 - val_loss: 1.2057 - val_acc: 0.5274\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/100\n",
      "180/180 [==============================] - 132s 734ms/step - loss: 0.4471 - acc: 0.8281 - val_loss: 1.2325 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/100\n",
      "180/180 [==============================] - 133s 736ms/step - loss: 0.4475 - acc: 0.8255 - val_loss: 1.2772 - val_acc: 0.5009\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/100\n",
      "180/180 [==============================] - 130s 721ms/step - loss: 0.4439 - acc: 0.8318 - val_loss: 1.3337 - val_acc: 0.5017\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Training Time:  2201.5210819244385\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "finetuned_model.fit_generator(\n",
    "    batches, \n",
    "    steps_per_epoch=num_train_steps, \n",
    "    epochs=100, \n",
    "    callbacks=[early_stopping, checkpointer], \n",
    "    validation_data=val_batches, \n",
    "    validation_steps=num_valid_steps)\n",
    "\n",
    "print(\"Training Time: \", (time.time() - start_time))\n",
    "\n",
    "\n",
    "finetuned_model.save('resnet50_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
