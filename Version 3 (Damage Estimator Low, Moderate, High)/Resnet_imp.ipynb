{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math, json, os, sys\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "print('Imports Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to training and validation data\n",
    "\n",
    "train_data_dir = 'dataset/train'\n",
    "validation_data_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params for CNN\n",
    "\n",
    "# img_width, img_height = 150, 150\n",
    "# batch_size = 5\n",
    "# epochs = 200\n",
    "# train_samples = 420\n",
    "# validation_samples = 80\n",
    "# img_channels = 3\n",
    "# cardinality = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'dataset'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'test')\n",
    "SIZE = (224, 224)\n",
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])\n",
    "num_valid_samples = sum([len(files) for r, d, files in os.walk(VALID_DIR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_steps = math.floor(num_train_samples/BATCH_SIZE)\n",
    "num_valid_steps = math.floor(num_valid_samples/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = keras.preprocessing.image.ImageDataGenerator()\n",
    "val_gen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 420 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = gen.flow_from_directory(TRAIN_DIR, target_size=SIZE, class_mode='categorical', shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_batches = val_gen.flow_from_directory(VALID_DIR, target_size=SIZE, class_mode='categorical', shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.applications.resnet50.ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = list(iter(batches.class_indices))\n",
    "model.layers.pop()\n",
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "last = model.layers[-1].output\n",
    "x = Dense(len(classes), activation=\"softmax\")(last)\n",
    "finetuned_model = Model(model.input, x)\n",
    "finetuned_model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "for c in batches.class_indices:\n",
    "    classes[batches.class_indices[c]] = c\n",
    "finetuned_model.classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10)\n",
    "checkpointer = ModelCheckpoint('resnet50_best.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 27s 318ms/step - loss: 0.6677 - acc: 0.6119 - val_loss: 0.9808 - val_acc: 0.5375\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98081, saving model to resnet50_best.h5\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 8s 101ms/step - loss: 0.5437 - acc: 0.7286 - val_loss: 0.9552 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98081 to 0.95516, saving model to resnet50_best.h5\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 9s 103ms/step - loss: 0.4540 - acc: 0.7833 - val_loss: 0.6280 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.95516 to 0.62803, saving model to resnet50_best.h5\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 8s 89ms/step - loss: 0.4231 - acc: 0.8024 - val_loss: 0.9071 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 6s 69ms/step - loss: 0.4453 - acc: 0.7976 - val_loss: 0.8590 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 4s 54ms/step - loss: 0.3794 - acc: 0.8405 - val_loss: 0.7912 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.3587 - acc: 0.8548 - val_loss: 0.8306 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.3613 - acc: 0.8381 - val_loss: 0.7308 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.3508 - acc: 0.8381 - val_loss: 0.8328 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.3357 - acc: 0.8429 - val_loss: 0.7961 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.3410 - acc: 0.8429 - val_loss: 0.9308 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.2837 - acc: 0.8786 - val_loss: 0.7915 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.2909 - acc: 0.8786 - val_loss: 0.9440 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "finetuned_model.fit_generator(batches, steps_per_epoch=num_train_steps, epochs=100, callbacks=[early_stopping, checkpointer], validation_data=val_batches, validation_steps=num_valid_steps)\n",
    "finetuned_model.save('resnet50_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
