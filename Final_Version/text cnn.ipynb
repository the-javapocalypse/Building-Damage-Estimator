{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javapocalypse\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from numpy import asarray\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Utilities.model_visualization import model_to_png\n",
    "\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('tweets.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6131 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (3616, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (3616, 4)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['label']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3254, 250) (3254, 4)\n",
      "(362, 250) (362, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 5,080,804\n",
      "Trainable params: 5,080,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2928 samples, validate on 326 samples\n",
      "Epoch 1/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.1863 - acc: 0.9529 - val_loss: 1.1292 - val_acc: 0.6595\n",
      "Epoch 2/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.1096 - acc: 0.9747 - val_loss: 1.2472 - val_acc: 0.6595\n",
      "Epoch 3/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0767 - acc: 0.9836 - val_loss: 1.2571 - val_acc: 0.6626\n",
      "Epoch 4/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0560 - acc: 0.9850 - val_loss: 1.2508 - val_acc: 0.6718\n",
      "Epoch 5/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0395 - acc: 0.9908 - val_loss: 1.3706 - val_acc: 0.6779\n",
      "Epoch 6/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0258 - acc: 0.9952 - val_loss: 1.4308 - val_acc: 0.6748\n",
      "Epoch 7/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0189 - acc: 0.9959 - val_loss: 1.5117 - val_acc: 0.6810\n",
      "Epoch 8/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0177 - acc: 0.9949 - val_loss: 1.5399 - val_acc: 0.6963\n",
      "Epoch 9/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0151 - acc: 0.9962 - val_loss: 1.6023 - val_acc: 0.6779\n",
      "Epoch 10/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0143 - acc: 0.9973 - val_loss: 1.4927 - val_acc: 0.6779\n",
      "Epoch 11/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0101 - acc: 0.9990 - val_loss: 1.6689 - val_acc: 0.6810\n",
      "Epoch 12/100\n",
      "2928/2928 [==============================] - 11s 4ms/step - loss: 0.0114 - acc: 0.9980 - val_loss: 1.6463 - val_acc: 0.6718\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_acc', patience=4, min_delta=0.0001)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('lstm_text.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96619326 0.01906555 0.01218033 0.00256095]] dont know\n"
     ]
    }
   ],
   "source": [
    "new_complaint = ['RT #Earthquake in #kuwait Everyone is standing outside']\n",
    "seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = ['dont know','little','mild','severe']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    df = pd.read_excel('tweets.xlsx')\n",
    "    \n",
    "    # The maximum number of words to be used. (most frequent)\n",
    "    MAX_NB_WORDS = 50000\n",
    "    # Max number of words in each complaint.\n",
    "    MAX_SEQUENCE_LENGTH = 250\n",
    "    # This is fixed.\n",
    "    EMBEDDING_DIM = 100\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(df['text'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    Y = pd.get_dummies(df['label']).values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.load_weights('lstm_text.h5')\n",
    "\n",
    "    new_complaint = ['RT #Earthquake in #kuwait Everyone is standing outside']\n",
    "    seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    pred = model.predict(padded)\n",
    "    labels = ['Don\\'t know or can\\'t say','Little Damage','Mild Damage','Severe Damage']\n",
    "    high_conf = pred[0][3]\n",
    "    moderate_conf = pred[0][2]\n",
    "    low_conf = pred[0][1]\n",
    "    print(text, 'Classified as: '+labels[np.argmax(pred)])\n",
    "    return high_conf, low_conf, moderate_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT #Earthquake in #kuwait Everyone is standing outside Classified as: Little Damage\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.027757093, 0.82317704, 0.121885866)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('RT #Earthquake in #kuwait Everyone is standing outside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
