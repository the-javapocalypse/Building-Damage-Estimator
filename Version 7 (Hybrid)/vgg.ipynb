{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras import metrics\n",
    "from keras.layers.merge import concatenate\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Utilities.model_visualization import model_to_png\n",
    "\n",
    "from PIL import Image # used for loading images\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os # used for navigating to image path\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "import cv2\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.applications import VGG16\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import pickle\n",
    "from scipy import misc\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text          HARVEY AFTER DONNA KISSED HIM: https://t.co/mz...\n",
      "text_info                                       not_informative\n",
      "image_path    data_image/hurricane_harvey/14_9_2017/90825435...\n",
      "damage                                               irrelevant\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pd.read_pickle('dataset.pkl')\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pos = int(df['text'].count()*0.8)\n",
    "train = df[:split_pos]\n",
    "test = df[split_pos:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataColumn = 'text'\n",
    "labelColumn = 'text_info'\n",
    "\n",
    "tags = train[labelColumn]\n",
    "texts = train[dataColumn]\n",
    "\n",
    "tags_Y = test[labelColumn]\n",
    "texts_Y = test[dataColumn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 1000\n",
    "# preprocess\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags.astype(str))\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)\n",
    "mat_texts = tok.texts_to_matrix(texts,mode='count')\n",
    "\n",
    "# For testing data\n",
    "le_Y = LabelEncoder()\n",
    "tags_Y = le_Y.fit_transform(tags_Y.astype(str))\n",
    "tok_Y = Tokenizer(num_words=num_max)\n",
    "tok_Y.fit_on_texts(texts_Y)\n",
    "mat_texts_Y = tok.texts_to_matrix(texts_Y,mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn preproces\n",
    "max_len = 100\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n",
    "\n",
    "# For testing data\n",
    "cnn_texts_seq_Y = tok.texts_to_sequences(texts_Y)\n",
    "cnn_texts_mat_Y = sequence.pad_sequences(cnn_texts_seq_Y,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"text_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=4, verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_model():    # Pre Trained Embeddings\n",
    "    \n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open('Embeddings/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((len(tok.word_index) + 1, 100))\n",
    "    for word, i in tok.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    #text classifier\n",
    "    inputs = Input(shape=(100,))\n",
    "    e = Embedding(len(tok.word_index) + 1,\n",
    "                  100, \n",
    "                  weights=[embedding_matrix],\n",
    "                  input_length=max_len, \n",
    "                  trainable=False)(inputs)\n",
    "    x = Dropout(0.2)(e)\n",
    "    x = Conv1D(128,\n",
    "               3,\n",
    "               padding='valid',\n",
    "               activation='relu',\n",
    "               strides=1)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    hybrid_link = Dense(32, activation='relu', name='hybrid_link')(x)\n",
    "    x = Dense(1, activation='sigmoid', name='Text_Classifier')(hybrid_link)\n",
    "    text_classifier = x\n",
    "    #image classifier\n",
    "    IMAGE_SIZE = [224, 224]  # we will keep the image size as (64,64). You can increase the size for better results. \n",
    "    vgg = VGG16(input_shape = (224, 224, 3), weights = None, include_top = True)  # input_shape = (64,64,3) as required by VGG\n",
    "    x = (vgg.layers[-2].output)\n",
    "    image_model = Dense(4, name='vgg_output', activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n",
    "    #hybrid model\n",
    "    concatenate_layer = concatenate([image_model, hybrid_link]) \n",
    "    hybrid = Dense(4, activation='softmax', name='Hybrid_Classifier')(concatenate_layer)\n",
    "    model = Model(inputs=[vgg.input, inputs], outputs=[hybrid,text_classifier])\n",
    "    \n",
    "    model.load_weights('initial_hybrid.h5')\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.layers.pop()\n",
    "    model.summary()\n",
    "    model = Model(inputs=[vgg.input, inputs], outputs=[image_model,text_classifier])\n",
    "    plot_model(model, to_file='divided models.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     4390000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 100)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 128)      38528       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 25088)        0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc1 (Dense)                     (None, 4096)         102764544   flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc2 (Dense)                     (None, 4096)         16781312    fc1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "vgg_output (Dense)              (None, 4)            16388       fc2[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 138,721,972\n",
      "Trainable params: 134,331,972\n",
      "Non-trainable params: 4,390,000\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     4390000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 100)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 128)      38528       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 25088)        0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc1 (Dense)                     (None, 4096)         102764544   flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fc2 (Dense)                     (None, 4096)         16781312    fc1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "hybrid_link (Dense)             (None, 32)           4128        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "vgg_output (Dense)              (None, 4)            16388       fc2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Text_Classifier (Dense)         (None, 1)            33          hybrid_link[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 138,726,133\n",
      "Trainable params: 134,336,133\n",
      "Non-trainable params: 4,390,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hybrid_model = get_hybrid_model()\n",
    "hybrid_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_model():    # Pre Trained Embeddings\n",
    "    \n",
    "    #image classifier\n",
    "    IMAGE_SIZE = [224, 224]  # we will keep the image size as (64,64). You can increase the size for better results. \n",
    "    vgg = VGG16(input_shape = (224, 224, 3), weights = None, include_top = True)  # input_shape = (64,64,3) as required by VGG\n",
    "    x = (vgg.layers[-2].output)\n",
    "    image_model = Dense(4, activation = 'softmax',name='Hybrid_Classifier')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n",
    "    model = Model(inputs=[vgg.input], outputs=[image_model])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hybrid_model.compile(loss=['categorical_crossentropy','binary_crossentropy'],\n",
    "                       optimizer= optimizers.adam(lr=0.0001),\n",
    "                       metrics=['accuracy',metrics.mae, metrics.categorical_accuracy])\n",
    "# hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN Image</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE =224\n",
    "dataset_dir = 'H:/FYP DATASETS/FYP DATASETS/Crisis/'\n",
    "\n",
    "def load_img(img):\n",
    "    path = os.path.join(dataset_dir, img)\n",
    "    rows=224\n",
    "    columns=224\n",
    "    img= cv2.resize(cv2.imread(path,cv2.IMREAD_COLOR),(rows,columns),interpolation=cv2.INTER_CUBIC)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train.iterrows():\n",
    "    train.at[index,'image_path'] = load_img(row['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(damage):\n",
    "    # integer encode\n",
    "    damage = np.array(damage)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(damage)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saad\\AppData\\Local\\conda\\conda\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text          HARVEY AFTER DONNA KISSED HIM: https://t.co/mz...\n",
      "text_info                                       not_informative\n",
      "image_path    [[[141, 150, 137], [141, 150, 137], [141, 150,...\n",
      "damage                                               irrelevant\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "y = encode_label(train.iloc[:]['damage'])\n",
    "print(train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['irrelevant' 'severe_damage' 'mild_damage' 'little_or_no_damage']\n"
     ]
    }
   ],
   "source": [
    "print(train.damage.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train['image_path'].tolist()\n",
    "# no need to convert y to list as it is 1 dim encoding takes care of it\n",
    "train_images = np.array(train_images)\n",
    "train_text = np.array(train['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"divided_checkpoints_1.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_vgg_output_acc', verbose=1, save_best_only=True, mode='max', period=1)\n",
    "early_stopping = EarlyStopping(monitor='val_vgg_output_acc', min_delta=0, patience=3, verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "load_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10807 samples, validate on 3603 samples\n",
      "Epoch 1/40\n",
      "10807/10807 [==============================] - 397s 37ms/step - loss: 1.1823 - vgg_output_loss: 0.6508 - Text_Classifier_loss: 0.5314 - vgg_output_acc: 0.8043 - vgg_output_mean_absolute_error: 0.1614 - vgg_output_categorical_accuracy: 0.8043 - Text_Classifier_acc: 0.7416 - Text_Classifier_mean_absolute_error: 0.3559 - Text_Classifier_categorical_accuracy: 1.0000 - val_loss: 1.0716 - val_vgg_output_loss: 0.6152 - val_Text_Classifier_loss: 0.4564 - val_vgg_output_acc: 0.7996 - val_vgg_output_mean_absolute_error: 0.1755 - val_vgg_output_categorical_accuracy: 0.7996 - val_Text_Classifier_acc: 0.8010 - val_Text_Classifier_mean_absolute_error: 0.3070 - val_Text_Classifier_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_vgg_output_acc improved from -inf to 0.79961, saving model to divided_checkpoints_1.h5\n",
      "Epoch 2/40\n",
      "10807/10807 [==============================] - 287s 27ms/step - loss: 1.0593 - vgg_output_loss: 0.5909 - Text_Classifier_loss: 0.4685 - vgg_output_acc: 0.8062 - vgg_output_mean_absolute_error: 0.1497 - vgg_output_categorical_accuracy: 0.8062 - Text_Classifier_acc: 0.7868 - Text_Classifier_mean_absolute_error: 0.3070 - Text_Classifier_categorical_accuracy: 1.0000 - val_loss: 0.9897 - val_vgg_output_loss: 0.5496 - val_Text_Classifier_loss: 0.4400 - val_vgg_output_acc: 0.7996 - val_vgg_output_mean_absolute_error: 0.1357 - val_vgg_output_categorical_accuracy: 0.7996 - val_Text_Classifier_acc: 0.8010 - val_Text_Classifier_mean_absolute_error: 0.2916 - val_Text_Classifier_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_vgg_output_acc did not improve from 0.79961\n",
      "Epoch 3/40\n",
      "10807/10807 [==============================] - 259s 24ms/step - loss: 1.0041 - vgg_output_loss: 0.5565 - Text_Classifier_loss: 0.4476 - vgg_output_acc: 0.8073 - vgg_output_mean_absolute_error: 0.1416 - vgg_output_categorical_accuracy: 0.8073 - Text_Classifier_acc: 0.8002 - Text_Classifier_mean_absolute_error: 0.2900 - Text_Classifier_categorical_accuracy: 1.0000 - val_loss: 0.9730 - val_vgg_output_loss: 0.5388 - val_Text_Classifier_loss: 0.4342 - val_vgg_output_acc: 0.8038 - val_vgg_output_mean_absolute_error: 0.1350 - val_vgg_output_categorical_accuracy: 0.8038 - val_Text_Classifier_acc: 0.8074 - val_Text_Classifier_mean_absolute_error: 0.2875 - val_Text_Classifier_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_vgg_output_acc improved from 0.79961 to 0.80377, saving model to divided_checkpoints_1.h5\n",
      "Epoch 4/40\n",
      "10807/10807 [==============================] - 255s 24ms/step - loss: 0.9809 - vgg_output_loss: 0.5448 - Text_Classifier_loss: 0.4361 - vgg_output_acc: 0.8082 - vgg_output_mean_absolute_error: 0.1387 - vgg_output_categorical_accuracy: 0.8082 - Text_Classifier_acc: 0.8051 - Text_Classifier_mean_absolute_error: 0.2797 - Text_Classifier_categorical_accuracy: 1.0000 - val_loss: 1.0088 - val_vgg_output_loss: 0.5758 - val_Text_Classifier_loss: 0.4330 - val_vgg_output_acc: 0.8115 - val_vgg_output_mean_absolute_error: 0.1682 - val_vgg_output_categorical_accuracy: 0.8115 - val_Text_Classifier_acc: 0.8085 - val_Text_Classifier_mean_absolute_error: 0.2897 - val_Text_Classifier_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_vgg_output_acc improved from 0.80377 to 0.81155, saving model to divided_checkpoints_1.h5\n",
      "Epoch 5/40\n",
      "10807/10807 [==============================] - 255s 24ms/step - loss: 0.9643 - vgg_output_loss: 0.5368 - Text_Classifier_loss: 0.4274 - vgg_output_acc: 0.8077 - vgg_output_mean_absolute_error: 0.1375 - vgg_output_categorical_accuracy: 0.8077 - Text_Classifier_acc: 0.8087 - Text_Classifier_mean_absolute_error: 0.2751 - Text_Classifier_categorical_accuracy: 1.0000 - val_loss: 0.9905 - val_vgg_output_loss: 0.5680 - val_Text_Classifier_loss: 0.4225 - val_vgg_output_acc: 0.8035 - val_vgg_output_mean_absolute_error: 0.1677 - val_vgg_output_categorical_accuracy: 0.8035 - val_Text_Classifier_acc: 0.8163 - val_Text_Classifier_mean_absolute_error: 0.2752 - val_Text_Classifier_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_vgg_output_acc did not improve from 0.81155\n",
      "Epoch 6/40\n",
      "10807/10807 [==============================] - 256s 24ms/step - loss: 0.9446 - vgg_output_loss: 0.5235 - Text_Classifier_loss: 0.4211 - vgg_output_acc: 0.8089 - vgg_output_mean_absolute_error: 0.1344 - vgg_output_categorical_accuracy: 0.8089 - Text_Classifier_acc: 0.8124 - Text_Classifier_mean_absolute_error: 0.2697 - Text_Classifier_categorical_accuracy: 1.0000 - val_loss: 0.9581 - val_vgg_output_loss: 0.5399 - val_Text_Classifier_loss: 0.4182 - val_vgg_output_acc: 0.7996 - val_vgg_output_mean_absolute_error: 0.1419 - val_vgg_output_categorical_accuracy: 0.7996 - val_Text_Classifier_acc: 0.8135 - val_Text_Classifier_mean_absolute_error: 0.2684 - val_Text_Classifier_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_vgg_output_acc did not improve from 0.81155\n",
      "Epoch 7/40\n",
      "10807/10807 [==============================] - 256s 24ms/step - loss: 0.9259 - vgg_output_loss: 0.5096 - Text_Classifier_loss: 0.4163 - vgg_output_acc: 0.8130 - vgg_output_mean_absolute_error: 0.1313 - vgg_output_categorical_accuracy: 0.8130 - Text_Classifier_acc: 0.8163 - Text_Classifier_mean_absolute_error: 0.2652 - Text_Classifier_categorical_accuracy: 1.0000 - val_loss: 0.9551 - val_vgg_output_loss: 0.5393 - val_Text_Classifier_loss: 0.4159 - val_vgg_output_acc: 0.8093 - val_vgg_output_mean_absolute_error: 0.1262 - val_vgg_output_categorical_accuracy: 0.8093 - val_Text_Classifier_acc: 0.8127 - val_Text_Classifier_mean_absolute_error: 0.2697 - val_Text_Classifier_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_vgg_output_acc did not improve from 0.81155\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "hybrid_history = hybrid_model.fit(x=[train_images,cnn_texts_mat], y=[y,tags],\n",
    "                           epochs=40,\n",
    "                           batch_size=20,\n",
    "                           validation_split=0.25,\n",
    "                           shuffle=True,\n",
    "                           callbacks = callbacks_list,\n",
    "                           verbose=1)\n",
    "hybrid_time = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# vgg_history = vgg_model.fit(x=[train_images], y=[y],\n",
    "#                            epochs=40,\n",
    "#                            batch_size=10,\n",
    "#                            validation_split=0.2,\n",
    "#                            shuffle=True,\n",
    "#                            verbose=1)\n",
    "# vgg_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid train time 2057.2990221977234\n",
      "load time 759.0337851047516\n"
     ]
    }
   ],
   "source": [
    "print('Hybrid train time ' + str(hybrid_time))\n",
    "print('load time ' + str(load_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model.save_weights('divided.h5')\n",
    "#vgg_model.save_weights('vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
