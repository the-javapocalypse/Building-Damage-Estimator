{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = \"Dataset1/nepal/train.xlsx\"\n",
    "dataset_test = \"Dataset1/nepal/test.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_excel(dataset_train)\n",
    "test = pd.read_excel(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train['text'][0]\n",
    "# train['label'][0]\n",
    "dataColumn = 'text'\n",
    "labelColumn = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweet_id                                               text  \\\n",
      "0  591902739002560000  #Nepals prime minister addressed country 1st t...   \n",
      "1  592939706788216064            So we read friends blog Lamjung working   \n",
      "2  592591542168252032                           Lend helping hand #Nepal   \n",
      "3  591903009279384960  theyve managed reach Kathmandu help guide  off...   \n",
      "4  592099765271198976  Israel Sending Aid Teams Nepal After Quake Isr...   \n",
      "\n",
      "      label  \n",
      "0  relevant  \n",
      "1  relevant  \n",
      "2  relevant  \n",
      "3  relevant  \n",
      "4  relevant  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javapocalypse\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "\n",
    "tags = train[labelColumn]\n",
    "texts = train[dataColumn]\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 2. 0. ... 0. 0. 0.]]\n",
      "(6998,) (6998, 1000)\n"
     ]
    }
   ],
   "source": [
    "num_max = 1000\n",
    "# preprocess\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags.astype(str))\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)\n",
    "mat_texts = tok.texts_to_matrix(texts,mode='count')\n",
    "print(tags[:5])\n",
    "print(mat_texts[:5])\n",
    "print(tags.shape,mat_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 821, 386, 107, 822, 71, 387, 3, 769, 121, 636]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35\n",
      " 821 386 107 822  71 387   3 769 121 636]\n",
      "(6998, 100)\n"
     ]
    }
   ],
   "source": [
    "# for cnn preproces\n",
    "max_len = 100\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "print(cnn_texts_seq[0])\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n",
    "print(cnn_texts_mat[0])\n",
    "print(cnn_texts_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_model(model,x,y):\n",
    "    model.fit(x,y,batch_size=32,epochs=10,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_model_v1():   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Javapocalypse\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 20)           20000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 20)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 64)            3904      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 40,801\n",
      "Trainable params: 40,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5598 samples, validate on 1400 samples\n",
      "Epoch 1/10\n",
      "5598/5598 [==============================] - 8s 1ms/step - loss: -7.3840 - acc: 0.4059 - binary_accuracy: 0.4059 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 2/10\n",
      "5598/5598 [==============================] - 1s 170us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 3/10\n",
      "5598/5598 [==============================] - 1s 193us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 4/10\n",
      "5598/5598 [==============================] - 1s 185us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 5/10\n",
      "5598/5598 [==============================] - 1s 166us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 6/10\n",
      "5598/5598 [==============================] - 1s 232us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 7/10\n",
      "5598/5598 [==============================] - 1s 202us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 8/10\n",
      "5598/5598 [==============================] - 1s 242us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 9/10\n",
      "5598/5598 [==============================] - 1s 225us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n",
      "Epoch 10/10\n",
      "5598/5598 [==============================] - 1s 235us/step - loss: -9.4151 - acc: 0.4073 - binary_accuracy: 0.4073 - val_loss: 0.0228 - val_acc: 0.9986 - val_binary_accuracy: 0.9986\n"
     ]
    }
   ],
   "source": [
    "m = get_cnn_model_v1()\n",
    "check_model(m,cnn_texts_mat,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
